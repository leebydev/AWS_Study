{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24caab09",
   "metadata": {},
   "source": [
    "## 하나의 TFRecord로 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c07947c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/kaggle_cat_dog/train/cat.12406.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/kaggle_cat_dog/train/cat.3199.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/kaggle_cat_dog/train/cat.3941.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/kaggle_cat_dog/train/dog.11143.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/kaggle_cat_dog/train/dog.12152.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>../data/kaggle_cat_dog/train/dog.9069.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>../data/kaggle_cat_dog/train/cat.11297.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>../data/kaggle_cat_dog/train/dog.3848.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>../data/kaggle_cat_dog/train/dog.2575.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>../data/kaggle_cat_dog/train/cat.12263.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         filename  label\n",
       "0      ../data/kaggle_cat_dog/train/cat.12406.jpg      0\n",
       "1       ../data/kaggle_cat_dog/train/cat.3199.jpg      0\n",
       "2       ../data/kaggle_cat_dog/train/cat.3941.jpg      0\n",
       "3      ../data/kaggle_cat_dog/train/dog.11143.jpg      1\n",
       "4      ../data/kaggle_cat_dog/train/dog.12152.jpg      1\n",
       "...                                           ...    ...\n",
       "24995   ../data/kaggle_cat_dog/train/dog.9069.jpg      1\n",
       "24996  ../data/kaggle_cat_dog/train/cat.11297.jpg      0\n",
       "24997   ../data/kaggle_cat_dog/train/dog.3848.jpg      1\n",
       "24998   ../data/kaggle_cat_dog/train/dog.2575.jpg      1\n",
       "24999  ../data/kaggle_cat_dog/train/cat.12263.jpg      0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 시작 (./cat_dog_train.tfrecords 파일생성)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7760141547f540c983bb729e94fee4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting 시작 (./cat_dog_valid.tfrecords 파일생성)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe4c596c2694c3fba3083a3f53f7a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "\n",
    "############ Tensorflow에서 제공된 Type별 Feature 생성 코드 ############\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    # string / byte 타입을 받아서 byte list를 리턴.\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    # float / double 타입을 받아서 float list를 리턴\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _floatarray_feature(array):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    # bool / enum / int / uint 타입을 받아서 int64 list를 리턴\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "############ DataFrame 생성 ############        \n",
    "    \n",
    "src = '../data/kaggle_cat_dog/train/'\n",
    "\n",
    "df = pd.DataFrame(os.listdir(src),\n",
    "                  columns=['filename'])\n",
    "\n",
    "df['label'] = ~df['filename'].str.contains('cat')\n",
    "\n",
    "df = df.astype({'label': 'int'})\n",
    "\n",
    "df['filename']  = df['filename'].map(lambda x : src + x)\n",
    "\n",
    "display(df)\n",
    "\n",
    "############ TFRecord 생성 함수 정의 ############        \n",
    "        \n",
    "def to_tfrecords(id_list, label_list, tfrecords_name):\n",
    "    # id_list : 이미지파일명을 가지고 있는 list\n",
    "    # label_list : 이미지파일의 label을 가지고 있는 list\n",
    "    # tfrecords_name : tfrecord의 이름(train,validation 구별을 위해필요)\n",
    "    \n",
    "    print(\"Converting 시작 (\" + os.path.join(tfrecords_name + '.tfrecords') + ' 파일생성)')\n",
    "    # GZIP으로 압축한 TFRecord 생성하기 위한 option\n",
    "    options = tf.io.TFRecordOptions(compression_type = 'GZIP')\n",
    "        \n",
    "    with tf.io.TFRecordWriter(path=os.path.join(tfrecords_name + '.tfrecords'), \n",
    "                              options=options) as writer:\n",
    "        \n",
    "        # tqdm은 반복문의 진행상황을 progressbar를 보여주는 모듈\n",
    "        # tqdm 파라미터\n",
    "\n",
    "        # iterable : 반복자 객체\n",
    "        # desc : 진행바 앞에 텍스트 출력\n",
    "        # total : int, 전체 반복량\n",
    "        # leave : bool, default로 True. (진행상태 잔상이 남음)\n",
    "        # ncols : 진행바 컬럼길이. width 값으로 pixel 단위로 보임.\n",
    "        # mininterval, maxinterval : 업데이트 주기. \n",
    "        #                            기본은 mininterval=0.1 sec, maxinterval=10 sec\n",
    "        # miniters : Minimum progress display update interval, in iterations.\n",
    "        # ascii : True로 하면 '#'문자로 진행바가 표시됨.\n",
    "        # initial : 진행 시작값. 기본은 0\n",
    "        # bar_format : str\n",
    "\n",
    "        # tqdm method\n",
    "        # clear() : 삭제\n",
    "        # refresh() : 강제 갱신\n",
    "        \n",
    "        for id_, label_ in tqdm(zip(id_list, label_list), \n",
    "                                total=len(id_list), \n",
    "                                position=0, \n",
    "                                leave=True):\n",
    "            image_path = id_\n",
    "            \n",
    "            # TFRecord를 생성하는 시점에 resize를 할수도 있고 TFRecord를 읽어서 DataSet을 만들때\n",
    "            # resize를 할 수 도 있다. \n",
    "            # resize 된 이미지를 저장할 경우 처리에 주의해야 한다. \n",
    "            # 잘못하는 경우 이미지 데이터가 깨져서 저장될 수 있음.\n",
    "            # 일반적으로 원본을 저장하고 사용할 때 resize해서 사용하는것이 일반적임.\n",
    "            \n",
    "            # 원본 이미지를 resize하지 않고 TFRecord로 저장.\n",
    "            _binary_image = tf.io.read_file(image_path)\n",
    "            \n",
    "            # Example 객체 생성\n",
    "            # 파이썬의 문자열은 모두 unicode. unicode 문자열을 byte배열로 바꾸는 함수가 encode()\n",
    "            # 각 type에 맞게 Feature 객체 생성 후 dict 생성. \n",
    "            # 이 dict를 이용해 Feauture 객체 생성 후 Example 객체 생성.\n",
    "            string_set = tf.train.Example(features=tf.train.Features(feature={\n",
    "                'image_raw': _bytes_feature(_binary_image),\n",
    "                'label': _int64_feature(label_)                \n",
    "            }))\n",
    "\n",
    "            # 만들어진 Example 객체를 binary string으로 변환한 후 파일에 저장\n",
    "            writer.write(string_set.SerializeToString())    \n",
    "            \n",
    "\n",
    "############ 데이터 분리 및 TFRecord 생성 함수 호출 ############        \n",
    "\n",
    "train_ids, val_ids, train_labels, val_labels = \\\n",
    "    train_test_split(df['filename'], \n",
    "                     df['label'], \n",
    "                     test_size=0.2, \n",
    "                     random_state=40, \n",
    "                     shuffle=True)\n",
    "\n",
    "\n",
    "to_tfrecords(train_ids, train_labels, './cat_dog_train')\n",
    "to_tfrecords(val_ids, val_labels, './cat_dog_valid')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f394839f",
   "metadata": {},
   "source": [
    "## 여러개의 TFRecord로 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4098102f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36545780c33143bb878ec1a08a215e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe413d497f046768487896d289db71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "############ Tensorflow에서 제공된 Type별 Feature 생성 코드 ############\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    # string / byte 타입을 받아서 byte list를 리턴.\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    # float / double 타입을 받아서 float list를 리턴\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _floatarray_feature(array):\n",
    "    # float / double 타입을 받아서 float list를 리턴\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=array))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    # bool / enum / int / uint 타입을 받아서 int64 list를 리턴\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "def to_example(filename):\n",
    "    # TFRecord를 생성하는 시점에 resize를 할수도 있고 TFRecord를 읽어서 DataSet을 만들때\n",
    "    # resize를 할 수 도 있다. \n",
    "    # resize 된 이미지를 저장할 경우 처리에 주의해야 한다. \n",
    "    # 잘못하는 경우 이미지 데이터가 깨져서 저장될 수 있음.\n",
    "    # 일반적으로 원본을 저장하고 사용할 때 resize해서 사용하는것이 일반적임.\n",
    "            \n",
    "    # 원본 이미지를 resize하지 않고 TFRecord로 저장.\n",
    "    image_string = tf.io.read_file(filename)    \n",
    "    \n",
    "    # './data/kaggle_cat_dog/train/cat.12406.jpg'\n",
    "    label = (filename.split('/')[-1]).split('.')[0] == 'dog'\n",
    "    # cat이면 False(0), dog이면 True(1)\n",
    "    \n",
    "    shape = tf.image.decode_jpeg(image_string).shape\n",
    "\n",
    "    feature = {\n",
    "        'image/height': _int64_feature(shape[0]),\n",
    "        'image/width': _int64_feature(shape[1]),\n",
    "        'image/channel': _int64_feature(shape[2]),\n",
    "        'image/label': _int64_feature(label),\n",
    "        'image/image_raw': _bytes_feature(image_string),\n",
    "        'image/filename': _bytes_feature(filename.encode())        \n",
    "    }\n",
    "    \n",
    "    # Example 객체 생성\n",
    "    # 파이썬의 문자열은 모두 unicode. unicode 문자열을 byte배열로 바꾸는 함수가 encode()\n",
    "    # 각 type에 맞게 Feature 객체 생성 후 dict 생성. \n",
    "    # 이 dict를 이용해 Feauture 객체 생성 후 Example 객체 생성.\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def chunkify(filename_list, n):\n",
    "    size = len(filename_list) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(filename_list[start:start + size])\n",
    "        start += size\n",
    "    results.append(filename_list[start:])\n",
    "    return results\n",
    "\n",
    "\n",
    "# tqdm은 반복문의 진행상황을 progressbar를 보여주는 모듈\n",
    "\n",
    "# tqdm 파라미터\n",
    "# iterable : 반복자 객체\n",
    "# desc : 진행바 앞에 텍스트 출력\n",
    "# total : int, 전체 반복량\n",
    "# leave : bool, default로 True. (진행상태 잔상이 남음)\n",
    "# ncols : 진행바 컬럼길이. width 값으로 pixel 단위로 보임.\n",
    "# mininterval, maxinterval : 업데이트 주기. \n",
    "#                            기본은 mininterval=0.1 sec, maxinterval=10 sec\n",
    "# miniters : Minimum progress display update interval, in iterations.\n",
    "# ascii : True로 하면 '#'문자로 진행바가 표시됨.\n",
    "# initial : 진행 시작값. 기본은 0\n",
    "# bar_format : str\n",
    "\n",
    "# tqdm method\n",
    "# clear() : 삭제\n",
    "# refresh() : 강제 갱신\n",
    "        \n",
    "def build_tfrecords(total_shards_num, split, filenames):\n",
    "    chunks = chunkify(filenames, total_shards_num)\n",
    "    failed = 0\n",
    "    for i, chunk in tqdm(enumerate(chunks),\n",
    "                         total=len(chunks),\n",
    "                         position=0,\n",
    "                         leave=True):        \n",
    "        tfrecords_path = './tfrecords/{}_{}_of_{}.tfrecords'.format(split,\n",
    "                                                                    str(i + 1).zfill(4),\n",
    "                                                                    str(total_shards_num).zfill(4))\n",
    "        # GZIP으로 압축한 TFRecord 생성하기 위한 option\n",
    "        # options = tf.io.TFRecordOptions(compression_type='GZIP')\n",
    "        \n",
    "        # with tf.io.TFRecordWriter(tfrecords_path, options=options) as writer:\n",
    "        with tf.io.TFRecordWriter(tfrecords_path) as writer:\n",
    "            for filename in chunk:\n",
    "                try:\n",
    "                    tf_example = to_example(filename)\n",
    "                    \n",
    "                    # 만들어진 Example 객체를 binary string으로 변환한 후 파일에 저장\n",
    "                    writer.write(tf_example.SerializeToString())\n",
    "                except:\n",
    "                    print(f'fail: {filename}')\n",
    "    \n",
    "    \n",
    "os.makedirs('tfrecords', exist_ok=True)\n",
    "filenames = tf.io.gfile.glob('../data/kaggle_cat_dog/train/*.jpg')\n",
    "\n",
    "train_data, valid_data = train_test_split(filenames, train_size=0.8)\n",
    "\n",
    "build_tfrecords(100, 'train', train_data)\n",
    "build_tfrecords(100, 'valid', valid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e2ee2f",
   "metadata": {},
   "source": [
    "## TFRecord이용해서 학습_여러개의 TFRecord를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7324c87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lab17/jupyter_home/code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "228af3b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f45169095c444edbbd7d69ed3a0b97e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function map_func at 0x7fad44048e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function map_func at 0x7fad44048e50> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 16:06:26.009391: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2022-04-22 16:06:26.027076: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2499995000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.58000, saving model to ./checkpoint/cat-dog-000001-0.580000-0.565000.hdf5\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.58000 to 0.72000, saving model to ./checkpoint/cat-dog-000002-0.720000-0.830000.hdf5\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.72000 to 0.76000, saving model to ./checkpoint/cat-dog-000003-0.760000-0.940000.hdf5\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.76000\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.76000 to 0.78000, saving model to ./checkpoint/cat-dog-000005-0.780000-0.990000.hdf5\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.78000\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.78000 to 0.84000, saving model to ./checkpoint/cat-dog-000007-0.840000-1.000000.hdf5\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.84000\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.84000 to 0.86000, saving model to ./checkpoint/cat-dog-000009-0.860000-1.000000.hdf5\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.86000\n",
      "\n",
      "Epoch 00001: val_acc improved from 0.86000 to 0.90000, saving model to ./checkpoint/cat-dog-000001-0.900000-0.850000.hdf5\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.90000\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.90000 to 0.92000, saving model to ./checkpoint/cat-dog-000003-0.920000-1.000000.hdf5\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "############### 기본학습 종료 ###############\n",
      "########### 재학습 진행 ###########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "215f74de1ca54a7bb955d2b0bc8363ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00001: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00002: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.92000\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.92000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# 학습에 필요한 DataSet 준비(여러개의 tfrecord 처리)\n",
    "\n",
    "# train, validation TFRecord 폴더 경로(여러개의 tfrecord 처리)\n",
    "# 폴더를 나누고 파일을 복사하는건 수동으로 처리\n",
    "train_tfrecord_path = './tfrecords/train'\n",
    "valid_tfrecord_path = './tfrecords/valid'\n",
    "\n",
    "train_tfrecord_list = tf.io.gfile.glob(train_tfrecord_path + '/*.tfrecords')\n",
    "valid_tfrecord_list = tf.io.gfile.glob(valid_tfrecord_path + '/*.tfrecords')\n",
    "\n",
    "# 읽어들인 TFRecord를 다음의 형태(dict)로 변환하는 함수\n",
    "# <ParallelMapDataset shapes: {id: (), image_raw: (), label: ()}, \n",
    "#                     types: {id: tf.string, image_raw: tf.string, label: tf.int64}>\n",
    "def _parse_image_function(example_proto):\n",
    "    # TFRecord를 읽어서 데이터를 복원하기 위한 자료구조.\n",
    "    image_feature_description = {\n",
    "        'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/channel': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'image/image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string)\n",
    "    }\n",
    "    return tf.io.parse_single_example(example_proto, \n",
    "                                      image_feature_description)\n",
    "\n",
    "# 위에서 얻은 ParallelMapDataset를 다음의 형태(shape)로 변환하는 함수\n",
    "# <ParallelMapDataset shapes: ((None, None, 3), ()), types: (tf.float32, tf.int64)>\n",
    "def map_func(target_record):      \n",
    "    img = target_record['image/image_raw']\n",
    "    label = target_record['image/label']\n",
    "    img = tf.image.decode_jpeg(img, channels=3)    \n",
    "    return img, label\n",
    "\n",
    "\n",
    "# 전처리(normalization & resize) 함수\n",
    "# 이미지 데이터 normalization\n",
    "# 우리예제는 TFRecord 생성 시 원본 size로 저장했기 때문에 image resize를 해야함.\n",
    "def image_preprocess_func(image, label):\n",
    "    result_image = image / 255\n",
    "    result_image = tf.image.resize(result_image, \n",
    "                                   (IMAGE_SIZE,IMAGE_SIZE),\n",
    "                                   antialias=False)   \n",
    "    return result_image, label\n",
    "\n",
    "\n",
    "# 만약 multinomial classification이면 one_hot처리도 필요함.\n",
    "def image_postprocess_func(image, label):\n",
    "#    onehot_label = tf.one_hot(label, depth=1049)    # binary인 경우 one_hot 사용안함.    \n",
    "    return image, label\n",
    "\n",
    "\n",
    "def make_dataset(tfrecords_path, is_train):\n",
    "    \n",
    "    dataset = tf.data.TFRecordDataset(tfrecords_path)\n",
    "\n",
    "    dataset = dataset.map(_parse_image_function,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.map(map_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "\n",
    "    dataset = dataset.map(image_preprocess_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.map(image_postprocess_func,\n",
    "                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    dataset = dataset.batch(BATCH_SIZE)\n",
    "\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "############### Parameter\n",
    "NUM_OF_TFRECORDS = 5 # 종류별 TFRecord의 개수\n",
    "BUFFER_SIZE = 16     # 데이터 shuffle을 위한 buffer size\n",
    "BATCH_SIZE = 8       # 배치 사이즈. 한번에 가져오는 이미지 데이터 개수 \n",
    "NUM_CLASS = 2        # class의 개수. binary인 경우는 필요없으며 categorical인 경우 설정\n",
    "IMAGE_SIZE = 150       \n",
    "\n",
    "############### Model\n",
    "\n",
    "input_layer = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3), name='input_layer')  \n",
    "\n",
    "## Pretrained Network\n",
    "pretrained_model = VGG16(weights='imagenet',\n",
    "                         include_top=False,\n",
    "                         input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),\n",
    "                         input_tensor=input_layer)\n",
    "\n",
    "pretrained_model.trainable = False\n",
    "\n",
    "x = pretrained_model.output\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(256,\n",
    "          activation='relu')(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "\n",
    "# EarlyStopping & Checkpoint & Learning Rate\n",
    "learning_rate_reduction=ReduceLROnPlateau(\n",
    "                        monitor= \"val_acc\", \n",
    "                        patience = 3, \n",
    "                        factor = 0.5, \n",
    "                        min_lr=1e-7,\n",
    "                        verbose=1)\n",
    "\n",
    "model_filename = './checkpoint/cat-dog-{epoch:06d}-{val_acc:0.6f}-{acc:0.6f}.hdf5'\n",
    "\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath=model_filename, \n",
    "    verbose=1, \n",
    "    save_freq='epoch', \n",
    "    save_best_only=True, \n",
    "    monitor='val_acc')\n",
    "\n",
    "es = EarlyStopping(monitor='val_acc', verbose=1, patience=5)\n",
    "\n",
    "# LearningRateScheduler 이용\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "\n",
    "# 데이터 세팅 후 학습(여러개(100개)의 tfrecord를 이용한 학습)\n",
    "for i in tqdm(range(NUM_OF_TFRECORDS),\n",
    "              total=NUM_OF_TFRECORDS,\n",
    "              position=0,\n",
    "              leave=True):\n",
    "    tfrecord_train_file = train_tfrecord_list[i]\n",
    "    tfrecord_valid_file = valid_tfrecord_list[i]\n",
    "    \n",
    "    dataset = make_dataset(tfrecord_train_file, True)\n",
    "    valid_dataset = make_dataset(tfrecord_valid_file, False)\n",
    "    \n",
    "    model.fit(dataset,\n",
    "              epochs=20,\n",
    "              validation_data=valid_dataset,\n",
    "#               callbacks = [checkpointer, es, learning_rate_reduction],\n",
    "              callbacks = [checkpointer],\n",
    "              verbose=0)       \n",
    "\n",
    "print('############### 기본학습 종료 ###############')    \n",
    "\n",
    "# 여기까지가 기본학습 처리입니다.\n",
    "\n",
    "# Pretrained Network 위에 새로운 Network을 추가합니다.\n",
    "# Base Network을 동결합니다.\n",
    "# 새로 추가한 Network을 학습합니다.\n",
    "\n",
    "\n",
    "# 아래의 작업이 추가로 필요합니다.\n",
    "# Base Network에서 일부 Layer의 동결을 해제합니다.\n",
    "# 동결을 해제한 층과 새로 추가한 층을 함께 학습합니다.\n",
    "\n",
    "pretrained_model.trainable = True\n",
    "\n",
    "for layer in pretrained_model.layers:\n",
    "    if layer.name in ['block5_conv1','block5_conv2','block5_conv3']:\n",
    "        layer.trainable = True\n",
    "    else:    \n",
    "        layer.trainable = False\n",
    "\n",
    "## learning rate를 줄이는게 일반적(미세조절)        \n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    \n",
    "## 재학습 진행\n",
    "print('########### 재학습 진행 ###########')\n",
    "\n",
    "for i in tqdm(range(NUM_OF_TFRECORDS),\n",
    "              total=NUM_OF_TFRECORDS,\n",
    "              position=0,\n",
    "              leave=True):\n",
    "\n",
    "    tfrecord_train_file = train_tfrecord_list[i]\n",
    "    tfrecord_valid_file = valid_tfrecord_list[i]\n",
    "    \n",
    "    dataset = make_dataset(tfrecord_train_file, True)\n",
    "    valid_dataset = make_dataset(tfrecord_valid_file, False)\n",
    "    \n",
    "    model.fit(dataset,\n",
    "              epochs=20,\n",
    "              validation_data=valid_dataset,\n",
    "#               callbacks = [checkpointer, es, learning_rate_reduction],\n",
    "              callbacks = [checkpointer],\n",
    "              verbose=0)            \n",
    "\n",
    "# 결과그래프(loss, accuracy)를 그리는것은 한번 고민해보시기 바랍니다.!!\n",
    "\n",
    "# 프로그램 수행환경 \n",
    "# Server : AWS EC2 2.4.1\n",
    "#          1 GPU(Tesla T4), 16GB GPU Memory\n",
    "# CUDA : 11.0.4, cuDNN : 8\n",
    "# Tensorflow-gpu : 2.4.1\n",
    " \n",
    "# 최종 저장된 모델\n",
    "\n",
    "# 5개의 TFRecord로 학습한 경우\n",
    "# Epoch 00016: val_acc improved from 0.95300 to 0.95400, \n",
    "# saving model to ./checkpoint/cat-dog-000016-0.954000-1.000000.hdf5\n",
    "\n",
    "# 1개의 TFRecord로 학습한 경우\n",
    "# Epoch 00015: val_acc improved from 0.95140 to 0.95420, \n",
    "# saving model to ./checkpoint/cat-dog-000015-0.954200-0.995150.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410a810a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:machine_TF2_17] *",
   "language": "python",
   "name": "conda-env-machine_TF2_17-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
